'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/ai_ml/','title':"AI and ML",'content':"Artificial Intelligence and Machine Learning #  Concepts and terminologies related to AI and ML are contained within this tree. Due to the presence of mathematical symbols page rendering might be delayed.\nHaving understanding and implications of mathematical concepts would be helpful in learning ML. I have gained knownledge from numerous blogs and videos that I couldn\u0026rsquo;t keep track of them all. I have added links to some the prominent links in the reference.\nReferences #    Youtube - 3Blue1Brown  Coursera  "});index.add({'id':1,'href':'/docs/software/data_structures/','title':"Data Structures",'content':"Data Structures #  https://cs.stackexchange.com/questions/49188/what-data-structure-is-best-suited-for-escape-analysis\nPrefix hash table\n"});index.add({'id':2,'href':'/docs/software/file_systems/','title':"File Systems",'content':"File systems #  "});index.add({'id':3,'href':'/docs/software/architecture/formal-methods/','title':"Formal Methods",'content':""});index.add({'id':4,'href':'/docs/software/networking/','title':"Networking",'content':"Networking #  "});index.add({'id':5,'href':'/docs/software/os/','title':"Operating Systems",'content':"File systems #  "});index.add({'id':6,'href':'/docs/software/languages/','title':"Programming",'content':"Programming #  Imperative programming #  Instruct the machine to perform certain operation in certain order.\nDeclarative Programming #  Such SQL, CSS(Selectors), we specify the data we what and the query optimizer decides how to get it.\nMapReduce Querying #  Programming model for processing large swathes of data across multiple machines.\n"});index.add({'id':7,'href':'/docs/software/protocols/','title':"Protocols",'content':"Protocols #  "});index.add({'id':8,'href':'/docs/software/security/','title':"Security",'content':"Security #  Securing data is of paramount importance to every corporation and household. Various options ranging from physical security to layers of security systems are installed to protect data. When it comes to security digital data the approach decided is depending on the state of data. Corporations restrict connections that are used to retrieve the data.\n"});index.add({'id':9,'href':'/docs/software/','title':"Software",'content':"Software #  "});index.add({'id':10,'href':'/docs/software/architecture/','title':"Software Architectures",'content':"Software Architectures #   Layered Architecture Event-Driven Architecture Microkernel Architecture Microservices Architecture Space-Based Architecture  References #    Oreilly Book  "});index.add({'id':11,'href':'/docs/software/standards/','title':"Standards",'content':"Standards and Conventions #  This document outlines the standards and convetions used to develop projects within this organization. These rules aren\u0026rsquo;t set on stone and can be modified in the future depending on the industry practices. However, the standards will not be changes frequently.\nFollowing conventions enable developers productivity. New developers joining the team can understand the codebase easier and follow them with ease with an established skleton.\n"});index.add({'id':12,'href':'/docs/ai_ml/supervised_learning/classification/','title':"Classification",'content':"Classification ML models #  Accuracy score doesn\u0026rsquo;t give correct measure in case of class imbalance. It doesn\u0026rsquo;t tell anything about the underlying class distribution. Cannot figure the type of error it is making. False positive, false negative etc.\nProbability generative models are based on P(X, Y) and discriminative models are based on P(X|Y)\nDiscriminative models learns soft and hard boundaries b/w classes Logistic Regression, Support Vector Machine and Conditional Random Fields are popular discriminative models\nGenerative models models the distribution of individual class Naive Bayes, Bayesian Networks and Hidden Markov models are commonly used generative models.\n"});index.add({'id':13,'href':'/docs/ai_ml/computer_vision/','title':"Computer Vision",'content':"Computer Vision #  "});index.add({'id':14,'href':'/docs/software/standards/database/','title':"Database Conventions",'content':"Database Conventions #  Models #   Network Model (CODASYL model) Relational Model Hierarchial Model Object Model XML based Model Document Model (MonogDB, CouchDB)  The choice of a datastore is determined by multitude of factors.\n Volume of read and write Storage considerations Response time for CRUD operations Access patterns Functionality provided Type and complexity of data stored Scalability Throughput  SQL #  SQL databases are well known for the ACID properties and NoSQL for the BASE. NoSQL databases are used almost everywhere where once SQL was unchallenged. For example, a bank transaction that is typically considered ACID are implemented using NoSQL storage solutions.\n"});index.add({'id':15,'href':'/docs/software/data_structures/distributed_data_structures/','title':"Distributed data structures",'content':"Distributed Data Structures #  https://www.microsoft.com/en-us/research/wp-content/uploads/2013/11/Tango.pdf\n"});index.add({'id':16,'href':'/docs/software/standards/event-sourcing/','title':"Event Sourcing",'content':"Event Sourcing #  Backward compatibility #  When formats of the message changes we have to ensure that the client depending on the previous format works. To overcome this, when a message payload is modified with breaking changes then we publish both version 1 format and the version 2 format of the message. This way clients depending on the newer format works. Different topics are created to publish different versions. Eventually when all clients migrate to the new form of the message, then the old one is dropped.\nVersioning #  "});index.add({'id':17,'href':'/docs/ai_ml/feature_engineering/','title':"Feature Engineering",'content':"Feature Engineering #  Frequentist vs Bayesian https://towardsdatascience.com/frequentist-vs-bayesian-approaches-in-machine-learning-86ece21e820e\n"});index.add({'id':18,'href':'/docs/software/standards/graph-ql/','title':"GraphQL Conventions",'content':"GraphQL Conventions #  References #    External-I  https://www.infoq.com/articles/GraphQL-ultimate-guide/?utm_source=email\u0026amp;utm_medium=editorial\u0026amp;utm_campaign=SpecialNL\u0026amp;utm_content=05202021\u0026amp;forceSponsorshipId=af7891c1-1a14-41dd-b773-8c18b614ce67\n"});index.add({'id':19,'href':'/docs/ai_ml/computer_vision/image_recognition/','title':"Image Recognition",'content':"Object recognition and Image Processing #  Techiniques,\n HOG - Histogram of Oriented Graphics SIFT - Scale Invariant Feature Transform CNN - Conventional Neural Network  "});index.add({'id':20,'href':'/docs/ai_ml/natural_language_processing/','title':"NLP",'content':"Natural Language Processing (NLP) #  "});index.add({'id':21,'href':'/docs/software/data_structures/probabilistic_data_structures/','title':"Probabilistic Structures",'content':"Probabilistic Data Structures #  "});index.add({'id':22,'href':'/docs/ai_ml/supervised_learning/regression/','title':"Regression",'content':"Regression classification models #  "});index.add({'id':23,'href':'/docs/software/standards/rest/','title':"REST-ful API Practices",'content':"REST-ful API Practices #  In REST the primary unit of identification is a resource that is accessed by a URI. A RESTful URI must convey the resource\u0026rsquo;s model to its clients.\nResource Naming conventions #   Use nouns instead of verbs. Nouns have properties and resources have attributes. Use plural nouns Multi-worded nouns  Considerations #  Different companies have used different strategies. There is no universal consensus on multi-worded nouns.\n Camel Case Underscores Hyphens  Projects in this repository follow hyphenated names. Since URLs are ideally case sensitive we avoid using it. So, between underscores and hyphens, we are going with hyphens as our choice of separators.\nNest Hierarchical objects #  Versioning #  By default, support the response with the most updated API version if none is requested. It is recommended that the clients explicitly request the required version trough the request headers.\nAccept: application/vnd.github.v3+json Backwards Compatibility #  API contract can change multiple times over the course of development. However, once a change is implemented in production breaking a contract is a grave offence. Nevertheless, contracts won\u0026rsquo;t be the same during the life of the application and changes are inevitable. Date and Time #  All date and time should be in ISO 8601 format.\n ISO 8601\nHTTP Verbs #     Verb Usage     HEAD Depending of application   GET Retrieving resource   POST Creating resources   PATCH Partial update of resource   PUT Replacing resource   DELETE Delete resource    HTTP Headers #  API First Approach #  Documentation #  OpenAPI\nAuthentication #  Authentication tokens must be passed through Authorization headers. Do not use query parameters.\nReferences #    External-I  Stack Overflow  Github  "});index.add({'id':24,'href':'/docs/software/data_structures/trees/','title':"Tree Structures",'content':"Tree Data Structures #  "});index.add({'id':25,'href':'/docs/ai_ml/supervised_learning/classification/evalutaion/','title':"Classification Model Evaluation",'content':"Evaluation Metrics - Classification Accuracy\nEvaluation of classification model #  Generative model Naive Bayes models the joint distribution of the feature X and target Y, and then predicts the posterior probability given as P(y|x). e.g. Naive Bayes, Bayesian Networks and Hidden Markov models\nDiscriminative model Logistic regression directly models the posterior probability of P(y|x) by learning the input to output mapping by minimising the error. e.g. Logistic Regression, Support Vector Machine and Conditional Random Fields\nTrue Positive Rate (or) Sensitivity (or) Recall #   Measure how good a model is at predicting positive class when the actual outcome is positive.    \\[TPR = {TP \\above{1pt} (TP \u0026#43; FN)}\\]  False Positive Rate (or) Inverse Specificity (or) False alarm rate #   Measure how ofter positive cases are prodicted when the outcome is negative.   \\[FPR = {1 - Specificity} \\\\ FPR = {FP \\above{1pt} (FP \u0026#43; TN)}\\]  Specificity #   \\[Specificity = {TN \\above{1pt} (TN \u0026#43; FP)}\\]  Precision #   Precision and Recall do not use TN   \\[Positive Predictive Power (or) Precision = {TP \\above{1pt} (TP \u0026#43; FP)}\\]  F-measure (or) F1-Score #   Harmonic mean of precision and recall  Akaike Information Criterion (AIC) #   Estimates quality of models for the same dataset relative to each other Used as a means for selecting correct model Lower the score better the model Usually used if less test data; Train on the entire dataset and use the AIC to validate model performance  Bayes Factor #  Precision Recall curves #   Used in binary classification Precision on y-axis and recall and x-axis for different thresholds Depicts trade-offs between TPR and positive predictive value using different probability thresholds Appropriate for imbalanced datasets  Receiver Operating Characteristic #   Metric used to evaulte classifier output quality. Depicts trade-offs between TPR and FPR using different probability thresholds True +ve rate on the y-axis and False +ve rate on the x-axis. Top right corner is the ideal value. Appropriate for balanced datasets Larger AUC (Area under the curve) is better Typically used in binary classifier, to use in multilabel classifier it is required to binarize the output.  Micro-averaging #   ROC alternate for mutilabel classifiers  References #    sklearn-I  External-I  "});index.add({'id':26,'href':'/docs/ai_ml/supervised_learning/classification/decision_trees/','title':"Decision Trees",'content':" Eager learners Works only in discrete data  "});index.add({'id':27,'href':'/docs/software/protocols/fix_protocol/','title':"FIX Protocol",'content':"Financial Information Exchange Protocol #  References #    Link-I  "});index.add({'id':28,'href':'/docs/ai_ml/supervised_learning/classification/kNN_classification/','title':"kNN Classifier",'content':"K Nearest Neighbour(kNN) Classifier #    Works with numeric data\n  Lazy learning\n  Instance based, lazy evaluation model i.e. store the data, at times with minimal processing, and evaulted once the test tuple is received\n  Approaches #    k-nearest neighbour approach - Instance represented as a point in euclidean space\n  Case-based reasoning - Uses symbolic representations and knowledge-based inferences\n  Target can be discrete or continuous\n  Nearest neighbour is based on distance computations\n  Consider all instances as members of n-dimensional space\n  Tuple Storage #  Brute Force\nKD Trees - K Dimensional trees\nBall Trees - Binary Search Trees\n Classification cost is high in kNN since the testing phase is computationally intensive compared to the training phase.   Lazy: Less time in training and more in predicting\n "});index.add({'id':29,'href':'/docs/ai_ml/supervised_learning/classification/naive_bayes/','title':"Naive Bayes Classifier",'content':" Assumes the features are independent and contributes to the predict (Class conditional assumption) Based on bayes theorem  H - Hypothesis E - Evidence P(H) - Prior Probability P(H|E) - Posterior Probability P(E|H) - Likelihood Ratio\nProbability of occurrence of hypothesis H given E is,\nP(H|E) = P(E|H) * P(H) / P(E)\nP(A|B) = P(A inter B) / P(B)\nP(B|A) = P(B inter A) / P(A)\n  \\[posterior = prior x likelihood\\]  Types #   Guassian - Normally distributed data Multinomial - Bernoulli - Feature vectors are binary  "});index.add({'id':30,'href':'/docs/ai_ml/supervised_learning/classification/support_vector_machines/','title':"Support Vector Machines",'content':"The shortest distance between the observations and the threshold is called the margin. When we use the threshold that gives the largest margin to make classifications it is call Maximum margin classifier.\n  Eager learners\n  Works for both linear and non-linear classification\n  Discriminative model\n  Leverages max margin classifier\n  Feature scaling\n  Distance based algorithm\n  Maximum margin classifier #  Sensitive to outliers\nLagrange Duality #  KKT Conditions #  Kernel Tricks #  References #    Blog-I  "});index.add({'id':31,'href':'/docs/ai_ml/bias_and_variance/','title':"Bias and Variance",'content':"Bias #  The difference between the data points and the fit line is bias.\nVariance #  The estimation diference across different set is variability\nIdeal model should have low bias and low variance. Simple and complex models are created to find the optimal bias and variance across test and training set.\nFinding optimal trade off #   Regularization Boosting Bagging  "});index.add({'id':32,'href':'/docs/ai_ml/diffusion_models/','title':"Diffusion Models",'content':"Diffusion Models #  Diffusion models are used in forecasting the future for a possibility without emprical knowledge.\n  BASS  "});index.add({'id':33,'href':'/docs/ai_ml/feature_engineering/data_transformation/','title':"Data Transformation",'content':"Data Transformation #   Normalization Attribute Construction Aggregation Attribute subset selection Discretization Generalization  Normalization #   Regression co-efficients depend on magnitude of feature variable Features with bigger magnitude overshadow the smaller ones Eculidian distance measure is sensitive to bigger mangnitude  Algorithm Sensitive to magnitude #   Linear and logistic regression Neural networks Support vector machines kNN K-Means clustering Linear discriminant analysis (LDA) Principal component analysis (PCA)  Types #   Min-Max normalization Z-Score normalization Decimal scaling normalization  Discretization #   Converting continuous values into discrete values   How to you choose number of patritions/bins Where to put the cut point  Depends on the problem being studied.\nUnsupervised Discretization #   Class label info not used Bins decided by experiment  Strategy,\n Equal interval binning - Outliers might skew Equal frequency binning - Same value might across different bins Binning by clustering  Heuristics,\n Number of intervals should not be smalled than the # of class number of features = number of samples / (3 * class types)  Supervised discretization #   Class label info is used Entropy based discretization Maximize purity of info i.e. contain less class mixture  "});index.add({'id':34,'href':'/docs/ai_ml/feature_engineering/feature_extraction/','title':"Feature Extraction",'content':"Transforms the orignal set of features into a into a different dimension of less features\nFeature creation #  Creating feature which has more info that the original dataset.\nMethodologies #    Feature extraction\n Domain specific In multimedia, low-level (pitch, tonality), mid-level (fourier, wavelet), high-level(tag, genre)    Mapping data to new space\n Fourier trans (freq domain) Wavelet trans (freq + time domain) Scale-Invariant Feature trans (SIFT) (Capute important/minuate points)    Feature construction\n Create dummy features  Converting categorical data to numerical (neural N/w works only on numerical)   Create dervived features  Customer time duration on a webpage      Tricks\n Handling Time, date and address difference Handling sales and marketing data; Create proportion like sales by marketing, by person Handling large volume of transaction - Document data given log transformation Special object as influencing person    "});index.add({'id':35,'href':'/docs/ai_ml/feature_engineering/feature_reduction/','title':"Feature Reduction",'content':"Feature Reduction #  Why? #  Too many features in a dataset complicates the model\u0026rsquo;s prediction strategy. Since most clustering models use some sort of distance measure, too many dimensions will result in many isolated clusters.\nHow many is too many? #  One indication is when there are too many features than the observations.\nHow? #   Principal Component Analysis Non-Negative Matrix Factorization Linear discriminant analysis t-SNE  References #  "});index.add({'id':36,'href':'/docs/ai_ml/feature_engineering/feature_selection/','title':"Feature Selection",'content':"Feature Selection #  Data sets that data scientists work upon arrives from multitude of sources. Hence, transforming data from these multiple sources to a common format is a mandatory step for deriving meaningful inference. Data will also have errors/deviations at the source of origin as well. We have a to make a careful consideration on how to handle such data points since these affect the learning model developed.\nThe extreme observations, when it influences the model performance are called influential points. The extremities are referred to as outliers or novelties depending on the context and it significance.\n Outlier Detection - Removing the extremities so as to ignore those points for model learning. Novelty Detection - Set of datapoints exist and we need to determine if a new observation fit into the distribution. Anamoly detection, financial frauds comes under novelty detection.   Select un-correlated features  Feature search strategy #   Optimum - Supervised and Unsupervised Heuristic - Forward, backward Randomized  Optimum Subset evaluation strategy #   Unsupervised Supervised  Algorithms #   Filter methods (Unsupervised) Wrapper Methods (Supervised) Embedded Methods  Filter Methods #   Apply statistical measure and score attributes accordingly. Based on the score drop or keep the feature Either univariate (no dependency amongst features) or with regards to dependent features  e.g. Chi-Squared test, information gain and correlation co-efficient scores\nWrapper methods #   Different subset of features are chosen and compared after evaluation Score is given based on model accuracy for the subset of features best-first search, hill climbing algorithm, backward and forward features  E.g. Recursive feature elimination algorithm\nEmbedded methods #   Learns which feature is contributes to best model accuracy at model creation time Common methods are regularization methods Regularization methods (also called penalization methods) introduces additional constraints  E.g. Lasso, Ridge and Elastic Net\nOutlier Detection #   Outlier Detection Methods\nUnivariate analysis #   Pearson correlation coefficient F score Chi Square Signal to noise ratio Mutual information  Box-Plot #  Scatter Plot #  IQR Score #  Cook\u0026rsquo;s Distance #  Z-Score #  Multivariate analysis #  Principal Component Analysis (PCA) #  Local Outlier Factor (LOF) #  High Contrast Subspaces for Density-Based Outlier Ranking (HiCS) #  Handling Outliers #  Drop Outlier #  Impute with Mean/Median #  Winsorizing #  Log-Scale Transformation #  Binnning #  Using different models #  Tree based models, such as random forests, are less impacted by the outliers. These models split the dataset into distinct, non-everlapping regions and compute the residual errors for each.\nUsing different loss functions to measure model performance #  Truncated loss function\nNovelty Detection #  References #   https://machinelearningmastery.com/an-introduction-to-feature-selection/  Scikit-learn  Quora  External-I  "});index.add({'id':37,'href':'/docs/ai_ml/measure/','title':"Measure",'content':"Evaluating model proficieny #  Devloping a machine learning model is one half of the coin, the other half however is measure the performance of the model and improving its efficiency. There are multidude ways of measuring the performance of model, most of them are generic in nature and can be used across mutliple models, and some are specifically designed towards specific learnging algorithms.\nCross Validation #   Statistical method to evalaute the model Lower bias  k-fold Cross Validation #   Evaluating models with limited set of data  Pseudocode,\n Split the data set into k parts Iterate through the parts and take one part as hold/test data set and the rest as train data set Evaluate the model for each split and record the values Repeat until all parts are taken as test/hold  Optimal k value #   Typically 5 or 10 Representative of the broader population k=n, each data sample is held and the rest is used for training (LOOCV)  Variations, #  Test/Train Split Split the data into 2 parts. One is for testing and the other is for training Leave one out Cross Validation (LOOCV) each sample is held and everthing else is used to train Stratified Splitting based on criteria; each fold has same proportion of observation Repeated Usual method of splitting into n folds\nReferences #    External-I  "});index.add({'id':38,'href':'/docs/ai_ml/natural_language_processing/concepts/','title':"Concepts",'content':"Concepts #  https://eng.uber.com/lsh/\nStemming - Stemming is the process of reducing inflection in words (e.g. troubled, troubles) to their root form (e.g. trouble). The “root” in this case may not be a real root word, but just a canonical form of the original word.\nThe most common algorithm, which is also known to be empirically effective for English, is (Porters Algorithm)[https://tartarus.org/martin/PorterStemmer/]\nbetter alternative - embeddings\nLemmatization Lemmatization on the surface is very similar to stemming, where the goal is to remove inflections and map a word to its root form. The only difference is that, lemmatization tries to do it the proper way. It doesn’t just chop things off, it actually transforms words to the actual root. For example, the word “better” would map to “good”.\nhttps://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html\n"});index.add({'id':39,'href':'/docs/ai_ml/supervised_learning/classification/logistic_regression/','title':"Logistic Regression",'content':"Logistic Regression #   Eager learners Logistic regression is a classification algorithm, that uses Sigmoid function. S-shaped curve varying between 0 and 1 and asymtotes at the tails Unlike linear regression the out vairable is discrete Logictic function takes any value and maps it between the range of 0 and 1 Can work on both continuous and discrete attributes When using multiple attributes we cannot compare models as such. Hence we remove/add attributes and check if the variableseffect on prediction is greater than zero. (Wald\u0026rsquo;s Test) The non helping attribute is referred as \u0026ldquo;totes useless\u0026rdquo; Unlike linear regression the residuals concept doesn\u0026rsquo;t apply here. Hence max likelihood is used to fit the curve Special type of Generalized Linear Models (GLM) Though presented as a logistic function, the coefficients are determined using liner function by converting to logit function.  Representation #    \\[h({\\theta}) = {1 \\above{1pt} (1 \u0026#43; e^-{\\theta^{T}x})} \\qquad \\text{e - natural logarithm base}\\]   \\[y \\isin \\lbrace 0, 1 \\rbrace \\qquad 0: \\text{Negative case; 1: Positive case }\\]  Interpretation #  The output of the hypothesis function is interpreted as the probability of the target bring positive given features x parameterized by  \\(\\theta\\)   \\[P(y=1|x;\\theta) \u0026#43; P(y=0|x;\\theta) = 1 \\\\ P(y=1|x;\\theta) = 1 - P(y=0|x;\\theta)\\]  Cost Function #   \\[J({\\theta}) = {1 \\above{1pt} m} \\sum_{\\substack{i = 0}}^m Cost(h_{\\theta}(x^i), y^i) \\qquad \\begin{dcases} {Cost(h_{\\theta}(x^i), y^i) = −log(h_{\\theta}(x))} \u0026amp;\\text{if y = 1} \\\\ {Cost(h_{\\theta}(x^i), y^i) = −log(1−h_{\\theta}(x)} \u0026amp;\\text{if y = 0} \\end{dcases}\\]  Cost function can be simplified as below; Substution y = 0 or 1 gives the same function above,  \\[Cost(h_{\\theta}(x^i), y^i) = -y * log(h_{\\theta}(x)) - (1 - y) * log(1 − h_{\\theta}(x)\\]   \\[J({\\theta}) = - {1 \\above{1pt} m} \\sum_{\\substack{i = 0}}^m y^i log(h_{\\theta}(x)) \u0026#43; (1 - y^i) log(1 - h_{\\theta}(x))\\]   Maximum likelihood estimation\n Interpretation #   Logistic regression models probability of the positive class.  Logit #    \\(logit^{-1}(x) = logistic(x)\\)   Find the y-intercept Find the standard-deviation z-value = y-intercept / standard-deviation; # of std deviations the y-intercept is away from mean of the normal curve Use Wald\u0026rsquo;s test to determine statistic significance.If the z-value is \u0026lt; 2 std dev, then insignificant  Optimization Algorithms #   Gradient Descedent Conjugate Gradient BFGS L-BFGS  Cross Entropy #  Unlike linear regression the prediction function(sigmoid function), the cost function has many local and global minima. Hence gradient descent cannot be used. Hence Cross entropy is used; also known as Loss function.\nReferences #    Why isn\u0026rsquo;t Logistic Regression called Logistic Classification?  "});index.add({'id':40,'href':'/docs/ai_ml/supervised_learning/regression/concepts/','title':"Concepts",'content':"Concepts #  Multicollinearity #  Phenomenon in which one predictor variable can be predicted from one another\n  Wikipedia  Homoscedasticity #  Error term being constant across all tuples\n  External  Heteroscedasticity #  Error term varies significantly across tuples. Ordinary Least Squares (OLS) gives equal weight to all observations. When heteroscedasticity is present Weighted least squares is to be used.\n"});index.add({'id':41,'href':'/docs/ai_ml/supervised_learning/regression/linear_regression/','title':"Linear Regression",'content':"Regression - Supervised Learning Algorithm #  Loss/Cost functions measure the correctness of the model prediction. Various methods are used depending on the requirement.\n Fit using residual least squares a.k.a sum of squares method i.e. the line that reduces the distance between the data points Using the residuals   \\(R^{2}\\)  is calcuated to compare simple and complex models (# of attributes used for prediction) Calcualte p-value for the calculated  \\(R^{2}\\)     \\(R^{2}\\)  #   Percentage of variation explained between two variables Ranges between 0 to 1. Higher the the value, better the model R is Co-efficient if correlation, which describe the relation between two variables say, x \u0026amp; y.  \\(R^{2}\\)  is the co-efficient of determiniation. Does not indicate the direction of corelation since squared numbers are always positive   \\[R^{2} = { variation(mean) - variation(best-fit-line) \\above{1pt} variation(mean) }\\]  Hypothesis #   \\[h(x) = \\theta_{0} \u0026#43; \\theta_{1}x\\]   \\[h(x) = \\theta^{T}x\\]  Cost function #   \\[J(\\theta_{0}, \\theta_{1}) = {1 \\above{1pt} 2m} \\sum_{\\substack{i = 0}}^m(\\hat y_{i} - y_{i})^{2}\\]  In terms of Theta,  \\[J(\\theta) = {1 \\above{1pt} 2m} \\sum_{\\substack{i = 0}}^m(\\hat y_{i} - y_{i})^{2} = {1 \\above{1pt} 2m} \\sum_{\\substack{i = 0}}^m(h(x_{i}) - y_{i})^{2}\\]  Mean Absolute Error (or) L1 Loss #  Mean of sum of absolute differences between actual and predicted values. Robust and not affected by outliers.\n \\[ mae = {1 \\above{1pt} m} \\sum_{\\substack{i = 0}}^m \\vert h(x_{i}) - y_{i} \\vert\\]  Mean Square Error (or) L2 Loss #  The mean of squared differences between the actual target and the prediction. Highly sensitive to outliers. W/O outliers, L2 performs better than L1.\n \\[ mse = {1 \\above{1pt} m} \\sum_{\\substack{i = 0}}^m(h(x_{i}) - y_{i})^{2}\\]  Huber Loss #  A combination of L1 and L2 loss functions. When the error with small compute L1 and when it is higher compute L2 loss. The hyper parameter  \\(\\delta\\)  determines when to switch to quadratic loss. The parameter  \\(\\delta\\)  is determined through iterative process.\n \\[L_{\\delta}(y, f(x)) = \\begin{dcases} {1 \\above{1pt} 2}(\\hat y_{i} - y_{i})^{2} \u0026amp;\\text{for } \\vert \\hat y_{i} - y_{i} \\vert \\leqq {\\delta} \\\\ {\\delta \\vert \\hat y_{i} - y_{i} \\vert} - {1 \\above{1pt} 2} \\delta^2 \u0026amp;\\text{otherwise } \\end{dcases}\\]  Log-Cosh Loss #  Log-Cosh is the log of hyperbolic cosine of the prediction error.\n \\[L(y, \\hat y) = \\sum_{\\substack{i = 0}}^m log(cosh(\\hat y - y))\\]  Quantile loss #  Truncated loss #  This functions trims the outliers to provide a more accurate prediction.\n \\[L(e) = \\begin{dcases} e^{2} \u0026amp;\\text{} \\forall \\{ e; Q_{\\substack{0.025}}(e) \u0026lt; e \u0026lt; Q_{\\substack{0.975}}(e) \\} \\\\ constant \u0026amp;\\text{otherwise} \\end{dcases}\\]  References #    External-I  TODO #  http://iacmc.zu.edu.jo/ar/images/stories/IACMC2016/39.pdf\n"});index.add({'id':42,'href':'/docs/ai_ml/terminologies/','title':"Terminologies",'content':"Probability #  Random experiment - An act that has predefined number of outcomes.\nElementary Event - Each individual possiblity in a random experiment. All possible elementary events of an experiment together make the the sample space of the experiment.\nRandom event - Combination of set/subset of elementary events, such as tosssing a coin multiple times.\nMarginal Probablity - Probablity of occurance of an event. This is probability in its most basic form. Probablity of an event A occuring is denoted as   \\(P(x = A)\\)  or simply  \\(P(A)\\)  .\nJoint Probablity - Probablity of simultaneous occurance of two random events, represented as  \\(P(X = A, Y = B)\\)  or  \\(P(A, B)\\)  or  \\(P(A \\cap B)\\)  .\nConditional Probability - Probability of an event occurring that another dependent event has occurend already. Represented as  \\(P(A \\mid B)\\)    Reference on conditional and joint probablity\n Lagrange Multiplier Used to convert constrained problem into an unconstraint problem. Works only on equality constraints.\nP-value #   Probability that the random chance generated the data, or something else that is equal or rarer. Value ranges between 0 and 1 Typical threshold is .05 or 5% Getting a small value in effect rejects the null hypothesis If the p-value is on the border line, then the data is inconclusive  Null Hypothesis #   Hypothesis that states that there is no relation between the attributes and the outcome. If p-value is less then 5%, then the null hypothesis is rejected and the alternate hypothesis is accepted  Distance computations #  Points Euclidian, Manhattan, Minkowski, Chevskov Binary Jaccard Similarity Doc clustering Cosine\nEuclidean - Not suitable for noisy data\nReferences #    Springer ML encyclopedia  "});index.add({'id':43,'href':'/docs/ai_ml/time_series_analysis/','title':"Time Series Analysis",'content':""});index.add({'id':44,'href':'/docs/ai_ml/time_series_analysis/models/','title':"Models",'content':"Classical / Statistical Models — Moving Averages, Exponential smoothing, ARIMA, SARIMA, TBATS Machine Learning — Linear Regression, XGBoost, Random Forest, or any ML model with reduction methods Deep Learning — RNN, LSTM "});index.add({'id':45,'href':'/docs/iot/lpwan/','title':"Lpwan",'content':"LPWAN #  https://docs.arduino.cc/foundations/wireless/low-power-wide-area-networks-101\n"});index.add({'id':46,'href':'/docs/software/algorithms/sorting_algorithms/','title':"Sorting Algorithms",'content':""});index.add({'id':47,'href':'/docs/software/data_structures/bitmap/','title':"Bitmap",'content':"BitMap #  Used in JFR (Java Flight Recorer for leak profiler)\nReferences #   https://dzone.com/articles/introduction-to-redis-data-structures-bitmaps  "});index.add({'id':48,'href':'/docs/software/data_structures/murmurhash/','title':"Murmurhash",'content':"MurMurHash #  Non-cryptographic hash function\n"});index.add({'id':49,'href':'/docs/software/data_structures/probabilistic_data_structures/bloom_filters/','title':"Bloom Filters",'content':"Bloom filters #  "});index.add({'id':50,'href':'/docs/software/data_structures/probabilistic_data_structures/hyperloglog/','title':"Hyperloglog",'content':"HyperLogLog #  "});index.add({'id':51,'href':'/docs/software/file_systems/clustered_file_systems/','title':"Clustered File Systems",'content':""});index.add({'id':52,'href':'/docs/software/file_systems/distributed_file_system/','title':"Distributed File System",'content':"Distributed File Systems #  https://en.wikipedia.org/wiki/Clustered_file_system#Distributed_file_systems\n  Lustre  https://en.wikipedia.org/wiki/GPFS\n"});index.add({'id':53,'href':'/docs/software/languages/if_vs_switch/','title':"If vs Switch",'content':"If vs Switch #  As we start learning any programming language if is the first conditional construct that we encounter. The construct may be expresssion or a statement, either way, it is an stepping stone towards the higher power. Often, we get a choice between if and switch and code reviwers would not likely agree with the choice.\nDuring my code review I consider the below to decide,\n More than 3 if/else condition should be a switch The expression evaluated has more than 1 condition if in case of OR \u0026amp; AND conditions in evaluation  There are alternatives to if and switch that are expressive and elegant.\nTernary operator #  Ternary operator is elegant for simple if else conditions. However, some languages to not support them and some has different syntax.\n$ (condition) ? \u0026lt;value to be returned if true\u0026gt; : \u0026lt;value to be returned if false\u0026gt; This (Tweet)[https://twitter.com/SimonHoiberg/status/1337688868527726593] response has few notable options available in Javascript.\n"});index.add({'id':54,'href':'/docs/software/languages/java/generics/','title':"Generics",'content':"Generics #  Proper bound should be set when generic programming is leveraged.\n Upper bound - \u0026lt;? extends Type\u0026gt; Lower bound - \u0026lt;? super Type\u0026gt;  "});index.add({'id':55,'href':'/docs/software/languages/java/jupiter/','title':"Jupiter",'content':"https://blogs.oracle.com/javamagazine/beyond-the-simple-an-in-depth-look-at-junit-5s-nested-tests-dynamic-tests-parameterized-tests-and-extensions?source=:em:nw:mt:::RC_WWMK200429P00043:NSL400084275\u0026amp;elq_mid=171707\u0026amp;sh=0822162609092617101828031334\u0026amp;cmid=WWMK200429P00043C0007\n"});index.add({'id':56,'href':'/docs/software/networking/bgp/','title':"Bgp",'content':"Border Gateway Protocol #  https://www.cloudflare.com/en-gb/learning/security/glossary/what-is-bgp/\n"});index.add({'id':57,'href':'/docs/software/os/Windows/tracing/','title':"Tracing",'content':"Tracing errors #   Run sxstrace.exe Trace -logfile:c:\\app.log Execute the file that you are tracing After running the application, end Trace Next parse the log file with sxstrace.exe Parse -logfile:c:\\app.log -outfile:c:\\app.txt  "});index.add({'id':58,'href':'/docs/software/security/data_security/','title':"Data Security",'content':"Data Security #  Securing Data at Rest #  Securing Data in Transit #  "});index.add({'id':59,'href':'/docs/software/security/security/','title':"Security",'content':"Security #   TLS Key exchange algorithms TLS Signature algorithms  Server decides which algorithm to use\nBGP with RPKI (Resource Public key infrastructure)\n   TPM - Temper Resistant Crypto chip\n Endorsement Key [By manufacturere]  linux seccomp\nhttps://www.infoq.com/presentations/reconcile-security-performance/?utm_source=notification_email\u0026amp;utm_campaign=notifications\u0026amp;utm_medium=link\u0026amp;utm_content=content_in_followed_topic\u0026amp;utm_term=daily\n"});index.add({'id':60,'href':'/docs/software/standards/database/data_locality/','title':"Data Locality",'content':"Google spanner DB locality features Oracle\u0026rsquo;s multi-table index cluster tables Bigtable\u0026rsquo;s column-family\n"});index.add({'id':61,'href':'/docs/software/standards/database/document_model/','title':"Document Model",'content':"Document Model #  Hierarchical structure of data nested within each other Access path for nested attributes is required Has same problem as RDBMS with respect to Joins. Data locality can be a problem for for entity. i.e. complete document is read and written every time. Some models provide sub-document lookup and updates (Couchbase)\n"});index.add({'id':62,'href':'/docs/software/standards/database/graph_data_model/','title':"Graph Data Model",'content':"Graph-like Data Models #  Suitable for many to many relationship models\nCalorich M ativan 1 mg *M\nliposerin concor 5 mg Renerve Bt 3\n"});index.add({'id':63,'href':'/docs/software/standards/serialization/','title':"Serialization",'content':"Serialization #   XML JSON Avro Protobuf Thrift Binary  http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html\n"});index.add({'id':64,'href':'/docs/software/standards/streams/mechanism/','title':"Mechanism",'content':"WebSocket #  WebTransport #  https://wicg.github.io/web-transport/\n"});index.add({'id':65,'href':'/readme/','title':"Readme",'content':"Add files to docs #  Add files to the docs directory for it to be visible in the website.\n"});index.add({'id':66,'href':'/docs/software/data_structures/trees/avl_trees/','title':"AVL Trees",'content':" Balanced BST   "});index.add({'id':67,'href':'/docs/software/data_structures/trees/b_trees/','title':"B-Trees",'content':"B-Trees #   Preferred structure to store indexes in disk Balanced tree  Metrics #  Depth for a tree of n keys it is O(log N)\nBranching factor #   Number of child references from each node of the Tree  Write amplification #  A write request that can amplify due to SSTable merge, WAL writes, Split of B-Tree node etc.\nUsages #   Used in Berkley DB [Initial version of chubby depended on it]  Optimisations #   Using WAL (Write ahead log) to survive crash if tree is being updated  "});index.add({'id':68,'href':'/docs/software/system_design/best_selling_rank/','title':"Best Seller Rank",'content':"Best Seller Rank #  Ranking best selling products in each category.\n"});index.add({'id':69,'href':'/docs/software/data_structures/trees/binary_search_trees/','title':"Binary Search Tree",'content':"Binary Search Trees #   Every node has atmost 2 children Left child is less than the parent Right child is greater than the parent Rotated when the tree is unbalanced  Why unbalanced is a problem? - O(~log N) is not possible in unbalanced tree\nBalanced BST #  AVL Trees Red-Black Trees\nOperations #   Typically soft deleted i.e. marked as deleted but the element stays  In-Order traversal - Left -\u0026gt; Root -\u0026gt; Right (recursive) The result will be ascending order\nPre-Order traversal - Root -\u0026gt; Left -\u0026gt; Right (recursive)\nPost-Order traversal - Left -\u0026gt; Right -\u0026gt; Root (recursive)\n"});index.add({'id':70,'href':'/docs/ai_ml/supervised_learning/classification/classification/','title':"Classification",'content':"Classification Algorithms #  Eager and Lazy Learning #  Eager learning models creates a model using the training data and use that on the test data for classification.\nLazy(or instance based) learning models simply stores the training data set and waits until the test dataset is given, e.g. kNN classifier. Less time in training and mode time in test.\nLazy learning approaches #   k-nearest neighbour Case-base reasoning  "});index.add({'id':71,'href':'/docs/ai_ml/unsupervised_learning/cluster_evaluation/','title':"Cluster Evaluation",'content':"Clustering #   Clustering tendency Number of clusters Clustering quality  Clustering tendency #   Hopkins test  Number of clusters #   Domain knowledge Data Driven  Elbow Gap Statistic Empirical (root of data-points)    Clustering quality #   Extrinsic measure Intrinsic measure  https://towardsdatascience.com/clustering-evaluation-strategies-98a4006fcfc\n"});index.add({'id':72,'href':'/docs/software/file_systems/content-addressable-file-system/','title':"Content addressable file systems",'content':"Content addressable file system #  e.x. Git\nhttps://git-scm.com/book/en/v2/Git-Internals-Git-Objects\n"});index.add({'id':73,'href':'/docs/software/data_structures/distributed_data_structures/crdt/','title':"CRDT",'content':"Conflict Free Replicated DataTypes #   G-Counter PN-Counter G-Set (Grow only Set) 2P-Set LWW-Element-Set OR-Set ORSWOT LWW-Map (Last Write Wins)  Primary Properties #   Cummutative Idempotent  "});index.add({'id':74,'href':'/docs/ai_ml/feature_engineering/data_preprocessing/','title':"Data Preprocessing",'content':"Why? #  Engineer the data suitable for building faster and simple models.\nSteps,\n Clean Format  Data Quality issues #   Missing values - Values not present. e.x. User\u0026rsquo;s unwillingness, non mandatory, storage issues, not applicable info, n/w \u0026amp; storage errors Duplicate data - Redundant data objects. Customer added new address, customers sharing address Inconsistent/Invalid data - Impossible values e.x. negative age, income, invalid zip code; Data entry errors Noise - Meaningless or invalid data; e.x. special chars Outliers - Data that is considerably away from the general behavior data  Data Wrangling #   Feature selection - Adding or removing feartures Feature transformation - Scaling, dimensionality reduction  Handling Missing values #  Impact #   Python libraries (scikit) incompatible with missing values Incorrect imputation may distort variable distribution Affects performance of model  Missingness Mechanism #  Finding why the data is missing would help decide the imputation logic.\nTypes,\n Missing completely at random (MCAR) - Probability of an instance missing the value is not dependent on the known values or the missing data Missing at Random (MAR) - Probability of missing values depend on the known values but not on the missing data itself Not missing at Random (NMAR/MNAR) - Probability of missing value could depend on the the value of the target variable  Missing completely at random (MCAR) #   Probability of data missing is same for all observations. No relationship b/w missing values and other attributes of the dataset Dropping ruch records will not affect inference  Missing at Random (MAR) #   Missing data in dependent on other attributes. Can impute values based on the other attribute  Not missing at Random (NMAR) #   Missing values exist as an indication of the target class  Imputation Techniques for numeric values #   Numerical Imputation - Mean/Median imputation, Random sampling imputation, Arbitary values imputation, Using values at the end of the distribution K-Nearest Neighbours (other variations - Fuzzy kNN, Sequential-kNN) Singular value decomposition (SVD) Baysean Principal component analysis (bPCA) Multiple imputations by chained equations (MICE) Expectation-Maximization (EM)  Imputation Techniques for categorical values #   Imputation by Mode   Note : Adding new attribute to indicate missingness is a practice followed\n Handling Duplicate data #   Delete duplicate data Merge duplicate data  External sources can be used to identify correct data.\nHandling Invalid data #   Use external knowledge bases or paid services Domain knowledge and common reasoning to come up with reasonalbe estimate  Handling Noise #   Filter out noise May result in partial loss of data if not done carefully  Handling Outliers #    Linear Regression, K-Nearest Neighbours and Adaboost are sensitive to outliers\n  Significantly skews distribution of data\n  Identified using summary stats and plots of data\n  In normal distribution ignore +/-3 standard deviations\n  In skewed use IQR\n  Median/Mode to the left of mean is a +ve skew.\nMean/Median Impuatation\n MCAR and MCR Assumes feature is normally distributed e.x. age Mean is sensitive to outliers and use median  Pros\n Easy to implement Easier to obtain complete dataset  Cons\n Reduces variance of the feature Does not preserve relationship b/w features i.e. corelation and covariance  Random Sampling Impuation\n MCAR and MCR Preserve the statistical parameter of the feature (mean and variance are not distorted)  Adding new feature\n MCAR and MCR Adds new feature  Impuation by tail values\n Used or NMAR Sort feature values and use the tail values for the missing values aka smoothing or binning  Imputation byu arbitary values\n Used for NMAR Chose au random value from the feature expect mean or median  Impuation for categorcial value Imput by Mode\n Used for NMAR  Encode missing value as a category\nAggregation #   Reduces variability in dataset and random roise Combining two or more attributes(e.x. height + weight = BMI) or two or more objects together (e.x. Grouping all male together)  Purpose #   Data Reduction Change of scale [Consider people names into males and female] More stable data [Reduces random noise]  Methods,\nContinues - Mean, Min. Max Discreate - Count, % of counts\nSampling #   Processing big datasets is expensive A representative sample is chosen to build model   Representative sample has approx same prop as the original population\n Simple random sampling #   When dataset is small go for w/o replacement   Sampling w/ replacement Sampling w/o replacement  Stratified sampling #   When class imbalance is present Group by class and then pick samples to ensure equal distribution  References #  https://www.hilarispublisher.com/open-access/a-comparison-of-six-methods-for-missing-data-imputation-2155-6180-1000224.pdf\nhttp://www.ukm.edu.my/jsm/pdf_files/SM-PDF-44-3-2015/17%20NuryAzmin.pdf\n5 - Label encoding\n Target encoding\n Outlier Detection - DB Scan\n"});index.add({'id':75,'href':'/docs/software/architecture/distributed_computing/','title':"Distributed Computing",'content':"Percentile Approximation #   Forward Decay hdrHistogram t-digest  In-Memory Data Grids #    Oracle Coherence  "});index.add({'id':76,'href':'/docs/software/data_structures/distributed_data_structures/distributed_hash_table/','title':"Distributed Hash Table",'content':"Distributed Hash Table #   No centralized server. Peer to peer  Popular Algorithms #   Chord Pastry Tapestry Kademlia  Tonika Routing\nFeross Aboukhadijeh\u0026rsquo;s talk\nhttps://learntla.com/introduction/\n"});index.add({'id':77,'href':'/docs/','title':"Docs",'content':""});index.add({'id':78,'href':'/docs/ai_ml/feature_engineering/feature_engineering/','title':"Feature Engineering",'content':"Feature Engineering #  The source of data available for Model training is vast, crude and usually not formatted. Various methods are employed to streamline the data to effectively train the model.\nFeature Selection #  In feature selection the input attributes are dropped or modified based on the model.\nDimensionality Reduction #  Dimensionality reduction creates new combination of attributes. e.g. Principal Component Analysis, Singular Value Decomposition and Sammon’s Mapping.\nReferences #   https://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf =======  Feature Engineering #  Data sets that data scientists work upon arrives from multitude of sources. Hence, transforming data from these multiple sources to a common format is a mandatory step for deriving meaningful inference. Data will also have errors/deviations at the source of origin as well. We have a to make a careful consideration on how to handle such data points since these affect the learning model developed.\nThe extreme observations, when it influences the model performance are called influential points. The extremeties are referred to as outliers or novelties depending on the context and it significance.\n Outlier Detection - Removing the extremeties so as to ignore those points for model learning. Novelty Detection - Set of datapoints exist and we need to determine if a new observation fit into the distribution. Anamoly detection, financial frauds comes under novelty detection.  Outlier Detection #   Outlier Detection Methods\nUnivariate analysis #  Box-Plot #  Scatter Plot #  IQR Score #  Cook\u0026rsquo;s Distance #  Z-Score #  Multivariate analysis #  Principal Component Analysis (PCA) #  Local Outlier Factor (LOF) #  High Contrast Subspaces for Density-Based Outlier Ranking (HiCS) #  Handling Outliers #  Drop Outlier #  Impute with Mean/Median #  Winsorizing #  Log-Scale Transformation #  Binnning #  Using different models #  Tree based models, such as random forests, are less impacted by the outliers. These models split the dataset into distinct, non-everlapping regions and compute the residual errors for each.\nUsing different loss functions to measure model performance #  Truncated loss function\nNovelty Detection #  References #    Scikit-learn  Quora  External-I  "});index.add({'id':79,'href':'/docs/software/data_structures/trees/fenwick_trees/','title':"Fenwick Trees",'content':"Fenwick Trees (or) Binary Indexed Trees #  Use cases #   Prefix sum of arrays  Complexity #  Space - O(N)\n   Operation Complexity     Create O(N * log N)   Update O(log N)        "});index.add({'id':80,'href':'/docs/software/data_structures/trees/fractal_trees/','title':"Fractal Trees",'content':"Fractal Trees #   Variant of B-Trees  "});index.add({'id':81,'href':'/docs/ai_ml/feature_engineering/gain_ratio/','title':"Gain Ratio",'content':"Gain Ratio #   Multi class result  "});index.add({'id':82,'href':'/docs/ai_ml/feature_engineering/gini_index/','title':"Gini Index",'content':"Gini Index #   Results in a binary value  "});index.add({'id':83,'href':'/docs/software/standards/http_3/','title':"HTTP/3",'content':"HTTP/3 #  https://quicwg.org/base-drafts/draft-ietf-quic-http.html\nhttps://www.infoq.com/news/2020/01/http-3-status/\n"});index.add({'id':84,'href':'/docs/ai_ml/feature_engineering/information_gain/','title':"Information Gain",'content':"Information Gain #   Multi class result  "});index.add({'id':85,'href':'/docs/software/file_systems/inter_planetary_file_system/','title':"Inter-Planetary file system (ISPF)",'content':""});index.add({'id':86,'href':'/docs/ai_ml/unsupervised_learning/k-means/','title':"K-Means Clustering",'content':"K-Means clustering #  Optimal K #   Elbow curve Silhoutte Score  "});index.add({'id':87,'href':'/docs/ai_ml/supervised_learning/regression/approach/','title':"Liner Regression Approach",'content':" Fit a line to the data set using least squares approach. Calculate R^2 Calculate p-value for the R^2  Calculating R^2\n Using the best-file line chosen using the least squares approach, we use test data to verify how good estimate the line provides.  Evaluation Metrics #   Mean Square Error Mean Absolute Error Root Mean Squared Error  "});index.add({'id':88,'href':'/docs/software/file_systems/log_structured_file_systems/','title':"Log structured file system",'content':""});index.add({'id':89,'href':'/docs/software/algorithms/longest_increasing_subsequence/','title':"Longest increasing subsequence",'content':""});index.add({'id':90,'href':'/docs/software/algorithms/max_sub_array_sum/','title':"Max Sum Subarray",'content':"Max Sum Subarray #   Implemented using Kadane\u0026rsquo;s algorithm in O(n). At each index determine the max subarray and take that as the current max to the next iteration  "});index.add({'id':91,'href':'/docs/software/networking/nat/','title':"NAT",'content':"Network Address Translation (NAT) #  Hole punching #   Alternative  "});index.add({'id':92,'href':'/docs/software/standards/database/no-sql/','title':"No-SQL",'content':"No-SQL Standards #  Selection #  Choice of No-SQL database must be carefully considered before using one. The right tool should be used for the right job.\n"});index.add({'id':93,'href':'/docs/software/system_design/parking_loft/','title':"Parking Loft",'content':""});index.add({'id':94,'href':'/docs/software/standards/database/rdbms/','title':"RDBMS",'content':"RDBMS #  Shredding Splitting an entity into multiple relationshsips [Person, Education, Experience]\nSchema on Write #  Strict enforcement of schema unlike NoSQL databases that has loose schema (Schema on Read).\nSchema Changes #  Schema changes are hard to change without downtime. Database migration tools must be used for easier maintance and roll backs.\n Liquibase Flyway  "});index.add({'id':95,'href':'/docs/ai_ml/reinforcement_learning/','title':"Reinforcement learning",'content':"   Reinforcement learning #  "});index.add({'id':96,'href':'/docs/software/data_structures/trees/segment_trees/','title':"Segment Trees",'content':"Use cases #   Range sub-queries i.e. min/max within range of values in array  "});index.add({'id':97,'href':'/docs/software/algorithms/sorting_algorithms/selection_sort/','title':"Selection Sort",'content':""});index.add({'id':98,'href':'/docs/software/data_structures/sstrees/','title':"Sorted String table",'content':"Sorted String Table (or) SSTrees #  "});index.add({'id':99,'href':'/docs/software/algorithms/sorting_algorithms/sorting_algorithms/','title':"Sorting algorithms",'content':"Properties #   Time complexity Space complexity Stability -\u0026gt; Relative ordering Recursive (or) Non-Recursive Internal (or) External  "});index.add({'id':100,'href':'/docs/software/data_structures/trees/splay_trees/','title':"Splay Trees",'content':"Splay Trees #  Use cases #   Used in cache. Recently accessed node is made the root  "});index.add({'id':101,'href':'/docs/software/standards/database/sql/','title':"SQL",'content':"SQL Conventions and Standards #  Migrations #  Schema transformation must be done through migrations. Following tools can be used,\n Liquibase Flyway  Overflowing primary key index - MySQL #  Issue faced at GitHub\n"});index.add({'id':102,'href':'/docs/software/standards/streams/','title':"Streams",'content':"Streams #  "});index.add({'id':103,'href':'/docs/ai_ml/supervised_learning/','title':"Supervised learning",'content':"   Supervised learning #  "});index.add({'id':104,'href':'/docs/software/system_design/','title':"System design",'content':"System design #  "});index.add({'id':105,'href':'/docs/software/architecture/formal-methods/tla/','title':"TLA+",'content':"https://cacm.acm.org/magazines/2015/4/184701-how-amazon-web-services-uses-formal-methods/fulltext\n"});index.add({'id':106,'href':'/docs/software/system_design/topk/','title':"Top K",'content':"Top K #  https://www.evernote.com/shard/s440/client/snv?noteGuid=db5d1f47-fc86-4bed-3323-46a19a029216\u0026amp;noteKey=53aa5fffcc236d3b7ef209ef5d916639\u0026amp;sn=https%3A%2F%2Fwww.evernote.com%2Fshard%2Fs440%2Fsh%2Fdb5d1f47-fc86-4bed-3323-46a19a029216%2F53aa5fffcc236d3b7ef209ef5d916639\u0026amp;title=Design%2Ba%2Bsystems%2Bwhich%2Bfinds%2BTop%2BK%2528Heavy%2BHitters%2529\n"});index.add({'id':107,'href':'/docs/ai_ml/unsupervised_learning/','title':"Unsupervised learning",'content':"   Supervised learning #  "});})();